{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=['Medical_Cost_Prediction.pdf']\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> \n",
      "\n",
      "4 \n",
      "\n",
      "<class 'llama_index.schema.Document'>\n",
      "Doc ID: 695ff974-3f00-482b-b3e4-8f279e583b82\n",
      "Text: 11/28/23, 11:00 PM README.md - Grip localhost:6419 1/4README.md\n",
      "Medical Cost Prediction Predicting medical costs of individuals based\n",
      "on different features using several ML (Regression) algorithms. The\n",
      "application was deployed on AWS EC2 through AWS ECR (Dockerized\n",
      "Container). Dataset The Medical Cost Prediction consists of around\n",
      "1300 records a...\n"
     ]
    }
   ],
   "source": [
    "print(type(documents), \"\\n\")\n",
    "print(len(documents), \"\\n\")\n",
    "print(type(documents[0]))\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline includes three components: Ingestion -> Retrieval -> Synthesis\n",
    "\n",
    "##### **Ingestion**: Documents are divided into chunks -> chuncks are embedded using embedding model -> the embeddings are stored in a Vector Store Index\n",
    "##### **Retrieval**: User's query is matched with the embeddings of chunks in index -> Top K chunks are taken out that match the embeddings of the query\n",
    "##### **Synthesis**: K chunks taken in the previous component are combined with the user's query -> Combined text (embeddings) is sent to the LLM for its response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the elements in the documents above\n",
    "from llama_index import Document\n",
    "\n",
    "document = Document(text='\\n\\n'.join([doc.text for doc in documents]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Store Index is used to store the chunks (from documents), text (chunk's text), and their corresponding embeddings\n",
    "# Service Context is used to change the default llm and embedding model used by llama index (it used OpenAI's llm and embedding model by default)\n",
    "from llama_index import VectorStoreIndex, ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model='gpt-3.5-turbo',temperature=0.1)\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model='local:BAAI/bge-small-en-v1.5')\n",
    "\n",
    "index = VectorStoreIndex([document], service_context=service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing query engine from the above index that allows us to send user's query to the **synthesis** component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The project is deployed on AWS EC2 through AWS ECR (Dockerized Container).\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query('How is the project deployed')\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation setup using TruLens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TruLens is a software tool that helps you to objectively measure the quality and effectiveness of your LLM-based applications using feedback functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the technologies used in the project?\n",
      "What is the installation process?\n",
      "Can you talk about the dataset used in the project?\n",
      "Can you explain the usage of the project?\n",
      "What is the process of deployment used in the project?\n",
      "What are the policies that needs to be attached to the IAM User?\n",
      "What are the packages that needs to be installed on EC2 isntance?\n",
      "What are the keys that needs to be initialized?\n",
      "Who is the author of the project?\n",
      "What license does the project use?\n"
     ]
    }
   ],
   "source": [
    "eval_questions = []\n",
    "with open('eval_questions.txt','r') as file:\n",
    "    for line in file:\n",
    "        # Removing new line character and converting it to integer\n",
    "        item = line.strip()\n",
    "        print(item)\n",
    "        eval_questions.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Tru, TruLlama\n",
    "tru = Tru()\n",
    "\n",
    "tru.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_recorder = TruLlama(query_engine, app_id='Direct Query Engine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_recorder as recording:\n",
    "    for question in eval_questions:\n",
    "        response = query_engine.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "records, feedback = tru.get_records_and_feedback(app_ids=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_id</th>\n",
       "      <th>app_json</th>\n",
       "      <th>type</th>\n",
       "      <th>record_id</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>tags</th>\n",
       "      <th>record_json</th>\n",
       "      <th>cost_json</th>\n",
       "      <th>perf_json</th>\n",
       "      <th>ts</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_tokens</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Direct Query Engine</td>\n",
       "      <td>{\"app_id\": \"Direct Query Engine\", \"tags\": \"-\",...</td>\n",
       "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
       "      <td>record_hash_a8d5d37c03de07b74e4ac073f683fb9a</td>\n",
       "      <td>\"What are the technologies used in the project?\"</td>\n",
       "      <td>\"The technologies used in the project include:...</td>\n",
       "      <td>-</td>\n",
       "      <td>{\"record_id\": \"record_hash_a8d5d37c03de07b74e4...</td>\n",
       "      <td>{\"n_requests\": 1, \"n_successful_requests\": 1, ...</td>\n",
       "      <td>{\"start_time\": \"2023-11-30T01:01:22.535105\", \"...</td>\n",
       "      <td>2023-11-30T01:01:27.735625</td>\n",
       "      <td>5</td>\n",
       "      <td>1045</td>\n",
       "      <td>0.001582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Direct Query Engine</td>\n",
       "      <td>{\"app_id\": \"Direct Query Engine\", \"tags\": \"-\",...</td>\n",
       "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
       "      <td>record_hash_b33498f62e6e5884ea63b21c69461f0e</td>\n",
       "      <td>\"What is the installation process?\"</td>\n",
       "      <td>\"To install the required packages for the Medi...</td>\n",
       "      <td>-</td>\n",
       "      <td>{\"record_id\": \"record_hash_b33498f62e6e5884ea6...</td>\n",
       "      <td>{\"n_requests\": 1, \"n_successful_requests\": 1, ...</td>\n",
       "      <td>{\"start_time\": \"2023-11-30T01:01:27.787209\", \"...</td>\n",
       "      <td>2023-11-30T01:01:37.866469</td>\n",
       "      <td>10</td>\n",
       "      <td>1113</td>\n",
       "      <td>0.001720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Direct Query Engine</td>\n",
       "      <td>{\"app_id\": \"Direct Query Engine\", \"tags\": \"-\",...</td>\n",
       "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
       "      <td>record_hash_211d0a15b287212c3ec5df33edb93270</td>\n",
       "      <td>\"Can you talk about the dataset used in the pr...</td>\n",
       "      <td>\"The dataset used in the project consists of a...</td>\n",
       "      <td>-</td>\n",
       "      <td>{\"record_id\": \"record_hash_211d0a15b287212c3ec...</td>\n",
       "      <td>{\"n_requests\": 1, \"n_successful_requests\": 1, ...</td>\n",
       "      <td>{\"start_time\": \"2023-11-30T01:01:37.908117\", \"...</td>\n",
       "      <td>2023-11-30T01:01:44.118351</td>\n",
       "      <td>6</td>\n",
       "      <td>1073</td>\n",
       "      <td>0.001637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Direct Query Engine</td>\n",
       "      <td>{\"app_id\": \"Direct Query Engine\", \"tags\": \"-\",...</td>\n",
       "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
       "      <td>record_hash_8e7ecf8cf73bcbc6cc529dfe0c8a323b</td>\n",
       "      <td>\"Can you explain the usage of the project?\"</td>\n",
       "      <td>\"The usage of the project involves several ste...</td>\n",
       "      <td>-</td>\n",
       "      <td>{\"record_id\": \"record_hash_8e7ecf8cf73bcbc6cc5...</td>\n",
       "      <td>{\"n_requests\": 1, \"n_successful_requests\": 1, ...</td>\n",
       "      <td>{\"start_time\": \"2023-11-30T01:01:44.169589\", \"...</td>\n",
       "      <td>2023-11-30T01:02:01.555708</td>\n",
       "      <td>17</td>\n",
       "      <td>1173</td>\n",
       "      <td>0.001838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Direct Query Engine</td>\n",
       "      <td>{\"app_id\": \"Direct Query Engine\", \"tags\": \"-\",...</td>\n",
       "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
       "      <td>record_hash_65808ba1c89a5102d1009e33727191b9</td>\n",
       "      <td>\"What is the process of deployment used in the...</td>\n",
       "      <td>\"The process of deployment used in the project...</td>\n",
       "      <td>-</td>\n",
       "      <td>{\"record_id\": \"record_hash_65808ba1c89a5102d10...</td>\n",
       "      <td>{\"n_requests\": 1, \"n_successful_requests\": 1, ...</td>\n",
       "      <td>{\"start_time\": \"2023-11-30T01:02:01.602333\", \"...</td>\n",
       "      <td>2023-11-30T01:02:22.927264</td>\n",
       "      <td>21</td>\n",
       "      <td>1193</td>\n",
       "      <td>0.001877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                app_id                                           app_json  \\\n",
       "0  Direct Query Engine  {\"app_id\": \"Direct Query Engine\", \"tags\": \"-\",...   \n",
       "1  Direct Query Engine  {\"app_id\": \"Direct Query Engine\", \"tags\": \"-\",...   \n",
       "2  Direct Query Engine  {\"app_id\": \"Direct Query Engine\", \"tags\": \"-\",...   \n",
       "3  Direct Query Engine  {\"app_id\": \"Direct Query Engine\", \"tags\": \"-\",...   \n",
       "4  Direct Query Engine  {\"app_id\": \"Direct Query Engine\", \"tags\": \"-\",...   \n",
       "\n",
       "                                                type  \\\n",
       "0  RetrieverQueryEngine(llama_index.query_engine....   \n",
       "1  RetrieverQueryEngine(llama_index.query_engine....   \n",
       "2  RetrieverQueryEngine(llama_index.query_engine....   \n",
       "3  RetrieverQueryEngine(llama_index.query_engine....   \n",
       "4  RetrieverQueryEngine(llama_index.query_engine....   \n",
       "\n",
       "                                      record_id  \\\n",
       "0  record_hash_a8d5d37c03de07b74e4ac073f683fb9a   \n",
       "1  record_hash_b33498f62e6e5884ea63b21c69461f0e   \n",
       "2  record_hash_211d0a15b287212c3ec5df33edb93270   \n",
       "3  record_hash_8e7ecf8cf73bcbc6cc529dfe0c8a323b   \n",
       "4  record_hash_65808ba1c89a5102d1009e33727191b9   \n",
       "\n",
       "                                               input  \\\n",
       "0   \"What are the technologies used in the project?\"   \n",
       "1                \"What is the installation process?\"   \n",
       "2  \"Can you talk about the dataset used in the pr...   \n",
       "3        \"Can you explain the usage of the project?\"   \n",
       "4  \"What is the process of deployment used in the...   \n",
       "\n",
       "                                              output tags  \\\n",
       "0  \"The technologies used in the project include:...    -   \n",
       "1  \"To install the required packages for the Medi...    -   \n",
       "2  \"The dataset used in the project consists of a...    -   \n",
       "3  \"The usage of the project involves several ste...    -   \n",
       "4  \"The process of deployment used in the project...    -   \n",
       "\n",
       "                                         record_json  \\\n",
       "0  {\"record_id\": \"record_hash_a8d5d37c03de07b74e4...   \n",
       "1  {\"record_id\": \"record_hash_b33498f62e6e5884ea6...   \n",
       "2  {\"record_id\": \"record_hash_211d0a15b287212c3ec...   \n",
       "3  {\"record_id\": \"record_hash_8e7ecf8cf73bcbc6cc5...   \n",
       "4  {\"record_id\": \"record_hash_65808ba1c89a5102d10...   \n",
       "\n",
       "                                           cost_json  \\\n",
       "0  {\"n_requests\": 1, \"n_successful_requests\": 1, ...   \n",
       "1  {\"n_requests\": 1, \"n_successful_requests\": 1, ...   \n",
       "2  {\"n_requests\": 1, \"n_successful_requests\": 1, ...   \n",
       "3  {\"n_requests\": 1, \"n_successful_requests\": 1, ...   \n",
       "4  {\"n_requests\": 1, \"n_successful_requests\": 1, ...   \n",
       "\n",
       "                                           perf_json  \\\n",
       "0  {\"start_time\": \"2023-11-30T01:01:22.535105\", \"...   \n",
       "1  {\"start_time\": \"2023-11-30T01:01:27.787209\", \"...   \n",
       "2  {\"start_time\": \"2023-11-30T01:01:37.908117\", \"...   \n",
       "3  {\"start_time\": \"2023-11-30T01:01:44.169589\", \"...   \n",
       "4  {\"start_time\": \"2023-11-30T01:02:01.602333\", \"...   \n",
       "\n",
       "                           ts  latency  total_tokens  total_cost  \n",
       "0  2023-11-30T01:01:27.735625        5          1045    0.001582  \n",
       "1  2023-11-30T01:01:37.866469       10          1113    0.001720  \n",
       "2  2023-11-30T01:01:44.118351        6          1073    0.001637  \n",
       "3  2023-11-30T01:02:01.555708       17          1173    0.001838  \n",
       "4  2023-11-30T01:02:22.927264       21          1193    0.001877  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dashboard ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd9c06d95c004ca4af956d47bd751887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(VBox(children=(Label(value='STDOUT'), Output())), VBox(children=(Label(valuâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard started at http://192.168.4.34:8501 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# launches on http://localhost:8501/\n",
    "tru.run_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
